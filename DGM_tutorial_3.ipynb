{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paulsubarna/FedWit/blob/main/vae_example_tutorial_curated_annotated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Oz6nZepLR6B"
      },
      "source": [
        "# Tutorial 3: Variational AutoEncoder\n",
        "\n",
        "In this tutorial, we will learn how to model and train our first generative latent variable model: Variational Autoencoders. It essentially connects the pieces that we had covered in our previous lectures if you recall. Happy fun Generating !!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98e9894d"
      },
      "source": [
        "## Imports and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmNtUi1kLR6F"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn import datasets\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from pytorch_model_summary import summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYx0O37sLR6H"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3f58647"
      },
      "source": [
        "\n",
        "This cell defines a PyTorch Dataset class for the Digits dataset, which contains images $x \\in \\mathbb{R}^{8 \\times 8}$ with 1500 samples and pixel values in $\\{0, 1, ..., 16\\}$. The dataset is partitioned into train/val/test splits. This setup prepares the data for easy batching while keeping the tutorial lightweight and runnable on CPUs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udWu0ENkLR6I"
      },
      "outputs": [],
      "source": [
        "class Digits(Dataset):\n",
        "    \"\"\"Scikit-Learn Digits dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, mode='train', transforms=None):\n",
        "        digits = load_digits()\n",
        "        if mode == 'train':\n",
        "            self.data = digits.data[:1000].astype(np.float32)\n",
        "        elif mode == 'val':\n",
        "            self.data = digits.data[1000:1350].astype(np.float32)\n",
        "        else:\n",
        "            self.data = digits.data[1350:].astype(np.float32)\n",
        "\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        if self.transforms:\n",
        "            sample = self.transforms(sample)\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3414629"
      },
      "source": [
        "**Probability Distributions**\n",
        "\n",
        "This cell implements log-likelihood functions for categorical, Bernoulli, and diagonal normal distributions, used for the VAE's likelihood and prior terms:\n",
        "- *Categorical log-likelihood*: $\\log p(x) = \\sum_i x_i \\log p_i$\n",
        "- *Bernoulli log-likelihood*: $\\log p(x) = x \\log p + (1-x)\\log(1-p)$\n",
        "- *Diagonal Normal*: $\\log \\mathcal{N}(x;\\mu,\\sigma^2) = -0.5 D \\log(2\\pi) - 0.5\\sum_j \\log \\sigma_j^2 - 0.5 \\sum_j \\frac{(x_j-\\mu_j)^2}{\\sigma_j^2}$\n",
        "\n",
        "The cell enables computation of log-probabilities essential for the VAE's loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKSSqDqKLR6J"
      },
      "outputs": [],
      "source": [
        "PI = torch.from_numpy(np.asarray(np.pi))\n",
        "EPS = 1.e-5\n",
        "\n",
        "def log_categorical(x, p, num_classes=256, reduction=None, dim=None):\n",
        "\n",
        "        return log_p\n",
        "\n",
        "def log_bernoulli(x, p, reduction=None, dim=None):\n",
        "\n",
        "        return log_p\n",
        "\n",
        "def log_normal_diag(x, mu, log_var, reduction=None, dim=None):\n",
        "\n",
        "        return log_p\n",
        "\n",
        "\n",
        "def log_standard_normal(x, reduction=None, dim=None):\n",
        "\n",
        "        return log_p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zluprX2ILR6K"
      },
      "source": [
        "## VAE Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6751872"
      },
      "source": [
        "**Encoder Module**\n",
        "\n",
        "Defines the encoder module $q_\\phi(z|x) = \\mathcal{N}(z;\\mu(x),\\sigma^2(x)I)$, outputting parameters $\\mu(x)$ and $\\log \\sigma^2(x)$ for each input. The *reparameterization trick* computes $z = \\mu + \\sigma \\odot \\epsilon$ for $\\epsilon \\sim \\mathcal{N}(0,I)$, which makes VAE optimization via stochastic gradient descent possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BWCik8DLR6K"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, encoder_net):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.encoder = encoder_net\n",
        "\n",
        "    @staticmethod\n",
        "    def reparameterization(mu, log_var):\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "\n",
        "\n",
        "        return\n",
        "\n",
        "    def sample(self, x=None, mu_e=None, log_var_e=None):\n",
        "\n",
        "        return z\n",
        "\n",
        "    def log_prob(self, x=None, mu_e=None, log_var_e=None, z=None):\n",
        "\n",
        "        return\n",
        "\n",
        "    def forward(self, x, type='log_prob'):\n",
        "\n",
        "        return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc770f25"
      },
      "source": [
        "## Decoder Module\n",
        "\n",
        "This cell sets up the VAE decoder $p_\\theta(x|z)$:\n",
        "- For categorical likelihood (image pixel classes), outputs are reshaped and a softmax produces pixel probabilities.\n",
        "- For Bernoulli likelihood, applies a sigmoid.\n",
        "Given a latent vector $z$, the decoder produces parameters for the likelihood $p_\\theta(x|z)$, reconstructing images from codes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrvkbAWcLR6L"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, decoder_net, distribution='categorical', num_vals=None):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.decoder = decoder_net\n",
        "        self.distribution = distribution\n",
        "        self.num_vals=num_vals\n",
        "\n",
        "    def decode(self, z):\n",
        "\n",
        "            return\n",
        "\n",
        "        elif self.distribution == 'bernoulli':\n",
        "\n",
        "            return\n",
        "\n",
        "        else:\n",
        "            raise ValueError('Either `categorical` or `bernoulli`')\n",
        "\n",
        "    def sample(self, z):\n",
        "\n",
        "\n",
        "        return x_new\n",
        "\n",
        "    def log_prob(self, x, z):\n",
        "\n",
        "        return log_p\n",
        "\n",
        "    def forward(self, z, x=None, type='log_prob'):\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f28a7933"
      },
      "source": [
        "**Prior Module**\n",
        "\n",
        "Implements the VAE prior $p(z) = \\mathcal{N}(0, I)$:\n",
        "- Sampling: $z \\sim \\mathcal{N}(0, I)$\n",
        "- Log-probability: Computes the log-density of $z$ under the standard normal.\n",
        "Intuition: Regularizes the latent space, forcing it to match a simple, tractable prior distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Df2c8gtLR6M"
      },
      "outputs": [],
      "source": [
        "class Prior(nn.Module):\n",
        "    def __init__(self, L):\n",
        "        super(Prior, self).__init__()\n",
        "        self.L = L\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "\n",
        "        return z\n",
        "\n",
        "    def log_prob(self, z):\n",
        "        return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5204ef0"
      },
      "source": [
        "## Full VAE, Loss, and Sampling\n",
        "\n",
        "Defines the complete VAE as a composition:\n",
        "\n",
        "**Forward pass**:\n",
        "- Encode $x$ to $ \\mu(x), \\log\\sigma^2(x) $\n",
        "- Sample $z$ using the reparameterization trick\n",
        "- Decode $z$ to reconstruct $x$ (e.g., BCE/MSE loss)\n",
        "- Compute the Evidence Lower Bound (ELBO):\n",
        "$$\n",
        "       \\mathcal{L}_{\\text{VAE}} = \\mathbb{E}_{q(z|x)}[\\log p(x|z)] - \\mathrm{KL}(q(z|x)||p(z))\n",
        "$$\n",
        "\n",
        "The KL term is for two diagonal Gaussians and the reconstruction from the chosen likelihood.\n",
        "\n",
        "### Sampling\n",
        "This cell draws $z\\sim\\mathcal{N}(0,I)$ and decodes to generate new samples $\\hat x\\sim p_\\theta(x\\mid z)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMA9CYlXLR6N"
      },
      "outputs": [],
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, encoder_net, decoder_net, num_vals=256, L=16, likelihood_type='categorical'):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        print('VAE by JT.')\n",
        "\n",
        "        self.encoder = Encoder(encoder_net=encoder_net)\n",
        "        self.decoder = Decoder(distribution=likelihood_type, decoder_net=decoder_net, num_vals=num_vals)\n",
        "        self.prior = Prior(L=L)\n",
        "\n",
        "        self.num_vals = num_vals\n",
        "\n",
        "        self.likelihood_type = likelihood_type\n",
        "\n",
        "    def forward(self, x, reduction='avg'):\n",
        "\n",
        "\n",
        "    def sample(self, batch_size=64):\n",
        "\n",
        "        return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52d06ae9"
      },
      "source": [
        "## Auxiliary Functions: Training, Evaluation, Plotting\n",
        "\n",
        "Implements utility functions for model training, validation/testing, and result visualization:\n",
        "- Computes and tracks loss (negative ELBO)\n",
        "- Saves models and generated samples\n",
        "- Plots the training curve, outputs images\n",
        "These tools facilitate model development and result inspection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXgrJc25LR6O"
      },
      "outputs": [],
      "source": [
        "def evaluation(test_loader, name=None, model_best=None, epoch=None):\n",
        "    # EVALUATION\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def samples_real(name, test_loader):\n",
        "    # REAL-------\n",
        "    num_x = 4\n",
        "    num_y = 4\n",
        "    x = next(iter(test_loader)).detach().numpy()\n",
        "\n",
        "    fig, ax = plt.subplots(num_x, num_y)\n",
        "    for i, ax in enumerate(ax.flatten()):\n",
        "        plottable_image = np.reshape(x[i], (8, 8))\n",
        "        ax.imshow(plottable_image, cmap='gray')\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.savefig(name+'_real_images.pdf', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def samples_generated(name, data_loader, extra_name=''):\n",
        "    x = next(iter(data_loader)).detach().numpy()\n",
        "\n",
        "    # GENERATIONS-------\n",
        "    model_best = torch.load(name + '.model')\n",
        "    model_best.eval()\n",
        "\n",
        "    num_x = 4\n",
        "    num_y = 4\n",
        "    x = model_best.sample(num_x * num_y)\n",
        "    x = x.detach().numpy()\n",
        "\n",
        "    fig, ax = plt.subplots(num_x, num_y)\n",
        "    for i, ax in enumerate(ax.flatten()):\n",
        "        plottable_image = np.reshape(x[i], (8, 8))\n",
        "        ax.imshow(plottable_image, cmap='gray')\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.savefig(name + '_generated_images' + extra_name + '.pdf', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_curve(name, nll_val):\n",
        "    plt.plot(np.arange(len(nll_val)), nll_val, linewidth='3')\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('nll')\n",
        "    plt.savefig(name + '_nll_val_curve.pdf', bbox_inches='tight')\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGnAVa5RLR6O"
      },
      "source": [
        "### Optimization setup\n",
        "This cell configures the optimizer and learning-rate schedule used to minimize $\\mathcal{L}_{\\text{VAE}}=\\text{ReconLoss}+\\mathrm{KL}$.\n",
        "\n",
        "### Training loop\n",
        "For each batch: encode $x\\to(\\mu,\\sigma)$, sample $z$, decode to $\\hat x$, compute losses, backprop, and update parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qpo4k1v9LR6O"
      },
      "outputs": [],
      "source": [
        "def training(name, max_patience, num_epochs, model, optimizer, training_loader, val_loader):\n",
        "    nll_val = []\n",
        "    best_nll = 1000.\n",
        "    patience = 0\n",
        "\n",
        "    # Main loop\n",
        "\n",
        "\n",
        "        # Validation\n",
        "\n",
        "\n",
        "        if patience > max_patience:\n",
        "            break\n",
        "\n",
        "    nll_val = np.asarray(nll_val)\n",
        "\n",
        "    return nll_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed6T7P8jLR6b"
      },
      "source": [
        "## Initialize dataloaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02c23815"
      },
      "source": [
        "**Hyperparameters**\n",
        "\n",
        "Specifies model hyperparameters:\n",
        "- Input dim $D$, latent dim $L$, network size $M$\n",
        "- Learning rate, epochs, early stopping\n",
        "These control model structure, speed, and regularization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COdDpLZjLR6c"
      },
      "outputs": [],
      "source": [
        "train_data = Digits(mode='train')\n",
        "val_data = Digits(mode='val')\n",
        "test_data = Digits(mode='test')\n",
        "\n",
        "training_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
        "\n",
        "result_dir = 'results/'\n",
        "if not(os.path.exists(result_dir)):\n",
        "    os.mkdir(result_dir)\n",
        "name = 'vae'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGA8wB2GLR6c"
      },
      "source": [
        "### Hyperparams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b663b12f"
      },
      "source": [
        "**Instantiate Model & Summaries**\n",
        "\n",
        "Chooses the likelihood type, configures encoder/decoder, and creates the VAE object. Model summaries help verify correct shapes and parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArgJInNqLR6d"
      },
      "outputs": [],
      "source": [
        "D = 64   # input dimension\n",
        "L = 16  # number of latents\n",
        "M = 256  # the number of neurons in scale (s) and translation (t) nets\n",
        "\n",
        "lr = 1e-3 # learning rate\n",
        "num_epochs = 1000 # max. number of epochs\n",
        "max_patience = 20 # an early stopping is used, if training doesn't improve for longer than 20 epochs, it is stopped"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yej8b56ELR6d"
      },
      "source": [
        "### Initialize VAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3416ca3"
      },
      "source": [
        "**Optimizer Setup**\n",
        "\n",
        "Configures the optimizer (Adamax) and sets the learning rate for updating model parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXxiGjUSLR6e"
      },
      "outputs": [],
      "source": [
        "likelihood_type = 'categorical'\n",
        "\n",
        "if likelihood_type == 'categorical':\n",
        "    num_vals = 17\n",
        "elif likelihood_type == 'bernoulli':\n",
        "    num_vals = 1\n",
        "\n",
        "encoder =\n",
        "decoder =\n",
        "\n",
        "prior =\n",
        "model = VAE(encoder_net=encoder, decoder_net=decoder, num_vals=num_vals, L=L, likelihood_type=likelihood_type)\n",
        "\n",
        "# Print the summary (like in Keras)\n",
        "print(\"ENCODER:\\n\", summary(encoder, torch.zeros(1, D), show_input=False, show_hierarchical=False))\n",
        "print(\"\\nDECODER:\\n\", summary(decoder, torch.zeros(1, L), show_input=False, show_hierarchical=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRds45K5LR6e"
      },
      "source": [
        "### Let's play! Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4V7fA6_ULR6e"
      },
      "source": [
        "### Optimization setup\n",
        "This cell configures the optimizer and learning-rate schedule used to minimize $\\mathcal{L}_{\\text{VAE}}=\\text{ReconLoss}+\\mathrm{KL}$.\n",
        "\n",
        "**How to verify it worked:** run without errors; if training, confirm the loss decreases across iterations/epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c195dfdf"
      },
      "source": [
        "**Training Procedure**\n",
        "\n",
        "Runs the training loop with early stopping. Periodically saves weights if validation loss improves and visualizes progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWJ0BVBcLR6e"
      },
      "outputs": [],
      "source": [
        "# OPTIMIZER\n",
        "optimizer = torch.optim.Adamax([p for p in model.parameters() if p.requires_grad == True], lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "904c0aa0"
      },
      "source": [
        "**Testing and Visualization**\n",
        "\n",
        "Evaluates the best model on the test set, saves loss and sample images, and learning curve plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNC96X4bLR6f"
      },
      "outputs": [],
      "source": [
        "# Training procedure\n",
        "nll_val = training(name=result_dir + name, max_patience=max_patience, num_epochs=num_epochs, model=model, optimizer=optimizer,\n",
        "                       training_loader=training_loader, val_loader=val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yvr0vzKDLR6f"
      },
      "source": [
        "\n",
        "**How to verify it worked:** run without errors; if training, confirm the loss decreases across iterations/epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVebjTH3LR6f"
      },
      "outputs": [],
      "source": [
        "test_loss = evaluation(name=result_dir + name, test_loader=test_loader)\n",
        "f = open(result_dir + name + '_test_loss.txt', \"w\")\n",
        "f.write(str(test_loss))\n",
        "f.close()\n",
        "\n",
        "samples_real(result_dir + name, test_loader)\n",
        "\n",
        "plot_curve(result_dir + name, nll_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0_qEbNMLR6g"
      },
      "source": [
        "## Visualization TO-DO\n",
        "\n",
        "### **Objective**\n",
        "Analyze and interpret the geometry of the learned latent space.\n",
        "\n",
        "### **Instructions**\n",
        "1. Select a dataset sample (images or other input) and encode it into the latent space using the VAE encoder.\n",
        "2. Apply a dimensionality reduction technique if needed (e.g., PCA or t-SNE) to project latent vectors to 2D.\n",
        "3. Visualize the resulting 2D latent representations.\n",
        "4. If labels are available, color points by class.\n",
        "5. Train the model with different number of latent variables\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UqcKK4FKoWhc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
